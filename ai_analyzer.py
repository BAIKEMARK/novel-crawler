# novel_crawler/ai_analyzer.py

import re
import json
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from config import get_chat_completion
from logger import logger

class AIAnalyzer:
    def __init__(self, model=None):
        self.model = model

    def _clean_html_for_ai(self, html: str) -> str:
        """
        åœ¨å°† HTML å‘é€ç»™ AI ä¹‹å‰è¿›è¡Œé¢„æ¸…ç†ï¼Œç§»é™¤å™ªéŸ³æ ‡ç­¾ã€‚
        è¿”å›æ¸…ç†åçš„ HTML å­—ç¬¦ä¸²ï¼ˆä¿ç•™ DOM ç»“æ„ï¼‰ã€‚
        """
        soup = BeautifulSoup(html, "html.parser")

        # ç§»é™¤æ‰€æœ‰å™ªéŸ³æ ‡ç­¾
        for tag in soup(
            [
                "script",
                "style",
                "header",
                "footer",
                "nav",
                "aside",
                "link", # ç§»é™¤ <link> æ ‡ç­¾
                "meta", # ç§»é™¤ <meta> æ ‡ç­¾
                "iframe", # ç§»é™¤ iframe
                ".read_menu",  # ç§»é™¤é˜…è¯»èœå•
                ".header",  # ç§»é™¤é¡¶éƒ¨å¯¼èˆª
                ".readPopup" # ç§»é™¤å¼¹çª—
            ]
        ):
            tag.decompose()

        # --- æ ¸å¿ƒä¿®å¤ï¼šè¿”å› HTML å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯ get_text() ---
        body = soup.find("body")
        if body:
            return str(body)  # è¿”å› body çš„ HTML ç»“æ„
        else:
            return str(soup)  # å›é€€åˆ°æ•´ä¸ª soup çš„ HTML ç»“æ„
        # --- ä¿®å¤ç»“æŸ ---

    def analyze_selectors(
        self,
        toc_html: str,
        chapter_html: str,
        domain: str,
        failed_rules: dict = None,
        last_error: str = None,
    ) -> dict:
        """
        ä½¿ç”¨ HTML åˆ†æç»“æ„ã€‚
        å¦‚æœæä¾›äº† failed_rules å’Œ last_errorï¼Œåˆ™è¿›å…¥â€œä¿®æ­£æ¨¡å¼â€ã€‚
        """

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šå…ˆæ¸…ç† HTML ---
        logger.info("ğŸ§ª æ­£åœ¨ä¸º AI æ¸…ç† HTML å™ªéŸ³...")
        cleaned_toc_html = self._clean_html_for_ai(toc_html)
        cleaned_chapter_html = self._clean_html_for_ai(chapter_html)

        # å®šä¹‰æˆªæ–­é•¿åº¦ï¼ˆæ¸…ç†åçš„æ–‡æœ¬å¯ä»¥æ›´é•¿ï¼‰
        MAX_LENGTH = 15000
        # --- ä¿®æ”¹ç»“æŸ ---

        if failed_rules and last_error:
            # â€œä¿®æ­£æ¨¡å¼â€çš„æç¤ºè¯
            prompt = f"""
ä½ æ˜¯å°è¯´ç½‘ç«™ç»“æ„åˆ†æä¸“å®¶ã€‚
ä½ ä¸Šæ¬¡ç”Ÿæˆçš„è§„åˆ™å¤±è´¥äº†ï¼Œè¯·ä½ ä¿®æ­£å®ƒã€‚

ã€ä¸Šæ¬¡å¤±è´¥çš„è§„åˆ™ã€‘
{json.dumps(failed_rules, indent=2, ensure_ascii=False)}

ã€å¤±è´¥åŸå› ã€‘
{last_error}

è¯·ä½ å‚è€ƒå¤±è´¥åŸå› ï¼Œé‡æ–°åˆ†æä¸‹é¢çš„ã€æ¸…ç†åçš„ HTMLã€‘ï¼Œå¹¶åªè¾“å‡ºä¿®æ­£åçš„ JSONï¼ˆä¸è¦åŠ è§£é‡Šï¼‰ï¼š

--------------------
ã€ç›®å½•é¡µ HTMLã€‘
{cleaned_toc_html[:MAX_LENGTH]}

--------------------
ã€ç« èŠ‚é¡µ HTMLã€‘
{cleaned_chapter_html[:MAX_LENGTH]}
            """
        else:
            # â€œåˆæ¬¡åˆ†æâ€çš„æç¤ºè¯
            prompt = f"""
ä½ æ˜¯å°è¯´ç½‘ç«™ç»“æ„åˆ†æä¸“å®¶ï¼Œä»¥ä¸‹æ˜¯ä¸¤ä¸ªã€æ¸…ç†åçš„ HTMLã€‘ï¼Œè¯·åˆ†æå…¶ç»“æ„ï¼Œå¹¶è¾“å‡ºé˜…è¯»å™¨ä¹¦æºé…ç½®ã€‚

- ğŸ“˜ ç¬¬ä¸€éƒ¨åˆ†ï¼šç›®å½•é¡µ HTMLï¼ˆåŒ…å«ç« èŠ‚åˆ—è¡¨ï¼‰
- ğŸ“„ ç¬¬äºŒéƒ¨åˆ†ï¼šç« èŠ‚é¡µ HTMLï¼ˆåŒ…å«ç« èŠ‚æ­£æ–‡ï¼‰

è¯·ç”Ÿæˆå¦‚ä¸‹ç»“æ„çš„ JSONï¼ˆä¸è¦åŠ è§£é‡Šï¼‰ï¼š

{{
  "bookSourceName": "{domain}ï¼ˆAIç”Ÿæˆï¼‰",
  "bookSourceUrl": "https://{domain}",
  "enabled": true,
  "bookSourceType": 0,
  "ruleToc": {{
    "chapterList": "CSSé€‰æ‹©å™¨ (ä¾‹å¦‚: #list > li > a)",
    "chapterName": "text",
    "chapterUrl": "href"
  }},
  "ruleContent": {{
    "content": "CSSé€‰æ‹©å™¨@textNodes##å¯é€‰æ¸…æ´—è§„åˆ™"
  }}
}}

--------------------
ã€ç›®å½•é¡µ HTMLã€‘
{cleaned_toc_html[:MAX_LENGTH]}

--------------------
ã€ç« èŠ‚é¡µ HTMLã€‘
{cleaned_chapter_html[:MAX_LENGTH]}
            """

        response_text = get_chat_completion(
            messages=[{"role": "user", "content": prompt}], model=self.model
        )

        try:
            match = re.search(r"```json\s*(\{.*\})\s*```", response_text, re.DOTALL)
            if not match:
                match = re.search(r"(\{.*\})", response_text, re.DOTALL)

            if not match:
                logger.error("âŒ AI è¿”å›å†…å®¹ä¸­æœªæ‰¾åˆ° JSON")
                logger.error(f"åŸå§‹AIå“åº”: {response_text[:500]}...")
                return None

            json_str = match.group(1)
            result = json.loads(json_str)

            if "ruleToc" in result and "ruleContent" in result:
                return result
            else:
                logger.error("âŒ è¿”å›ç¼ºå°‘ ruleToc æˆ– ruleContent")
                return None
        except Exception as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            logger.error(f"åŸå§‹AIå“åº”: {response_text[:500]}...")
            return None