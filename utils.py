# novel_crawler/utils.py

import re
import time
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import cloudscraper

# ğŸŒ åˆ›å»º scraper å®ä¾‹ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ + é˜²æŠ¤ç»•è¿‡ï¼‰
scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'chrome',
        'platform': 'windows',
        'mobile': False
    },
    delay=10
)

# ğŸ“¦ é»˜è®¤æµè§ˆå™¨ UA
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    )
}


def get_domain(url):
    """æå–ç«™ç‚¹ä¸»åŸŸå"""
    return urlparse(url).netloc


def fetch_html(url, retries=3, delay=2):
    """è·å–ç½‘é¡µ HTML å†…å®¹ï¼Œå¸¦é‡è¯•é€»è¾‘"""
    for attempt in range(retries):
        try:
            response = scraper.get(url, timeout=15, headers=DEFAULT_HEADERS)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or "utf-8"
            return response.text
        except Exception as e:
            print(f"[retry {attempt+1}/{retries}] è¯·æ±‚å¤±è´¥: {e}")
            time.sleep(delay)
    print(f"âŒ å¤šæ¬¡è¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡ï¼š{url}")
    return None


def parse_booksource_selector(selector: str):
    """
    è§£æä¹¦æºæ ¼å¼å­—ç¬¦ä¸²ï¼š
    e.g. 'article.article@textNodes##å¹¿å‘Š.+|è¯·å‹¿è½¬è½½'
    è¿”å›ï¼š
        css_selector, filter_type, [æ¸…æ´—è§„åˆ™...]
    """
    main_part, *filters = selector.split("##")
    if "@textNodes" in main_part:
        css_selector = main_part.replace("@textNodes", "")
        filter_type = "textNodes"
    elif "@text" in main_part:
        css_selector = main_part.replace("@text", "")
        filter_type = "text"
    else:
        css_selector = main_part
        filter_type = "text"

    return css_selector.strip(), filter_type, filters


def scrape_chapter(url, book_source):
    """æ ¹æ®ä¹¦æºè§„åˆ™æŠ“å–å•ç« å†…å®¹"""
    html = fetch_html(url)
    if not html:
        return None

    soup = BeautifulSoup(html, 'html.parser')

    # æå–æ ‡é¢˜
    try:
        title = soup.title.string.split("_")[0].strip()
    except:
        title = "æœªçŸ¥ç« èŠ‚"

    # æ­£æ–‡å†…å®¹æå–
    rule_content = book_source.get("ruleContent", {}).get("content", "")
    selector, ftype, filters = parse_booksource_selector(rule_content)

    content_el = soup.select_one(selector)
    if not content_el or not content_el.get_text(strip=True):
        print(f"âŒ æœªåŒ¹é…åˆ°æ­£æ–‡å†…å®¹ï¼Œé€‰æ‹©å™¨æ— æ•ˆï¼š{selector}")
        print(f"âš ï¸ é¡µé¢ç‰‡æ®µé¢„è§ˆ:\n{soup.prettify()[:500]}")
        content = "âš ï¸ æ­£æ–‡æŠ“å–å¤±è´¥"
    else:
        if ftype == "textNodes":
            # æ›¿æ¢ <br> ä¸ºæ¢è¡Œç¬¦
            for br in content_el.find_all("br"):
                br.replace_with("\n")
            content = content_el.get_text(separator="", strip=True)
        else:
            content = content_el.get_text(strip=True)

        # åº”ç”¨æ¸…æ´—è§„åˆ™
        for f in filters:
            content = re.sub(f, "", content)

    # ä¸‹ä¸€ç« é“¾æ¥ï¼ˆä¼˜å…ˆæ‰¾ "ä¸‹ä¸€ç« " æˆ– "ä¸‹ä¸€é¡µ"ï¼‰
    next_link = soup.find("a", string=re.compile(r"ä¸‹ä¸€[ç« é¡µ]"))
    next_url = urljoin(url, next_link['href']) if next_link and next_link.get("href") else None

    return {
        "title": title,
        "content": content,
        "next_url": next_url
    }
