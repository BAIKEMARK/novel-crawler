# novel_crawler/main.py

import os
import argparse
import time
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor

from utils import get_domain, fetch_html, scrape_chapter
from ai_analyzer import AIAnalyzer
from chapter_writer import ChapterWriter
from cleaner import clean_content
from booksource_loader import find_book_source, append_book_source


def main(start_url):
    domain = get_domain(start_url)
    print(f"[ğŸŒ] ç›®æ ‡ç«™ç‚¹ï¼š{domain}")

    # âœ… å°è¯•ä»ä¹¦æºä¸­åŠ è½½é…ç½®
    book_source = find_book_source(start_url)
    if not book_source:
        print("ğŸ“¡ æœªæ‰¾åˆ°ä¹¦æºï¼Œå¯åŠ¨ AI åˆ†æç»“æ„...")
        html = fetch_html(start_url)
        analyzer = AIAnalyzer()
        book_source = analyzer.analyze_selectors(html, domain)
        if not book_source:
            print("âŒ AI åˆ†æå¤±è´¥ï¼Œç»ˆæ­¢ã€‚")
            return
        append_book_source(book_source)
        print("ğŸ“¥ AIç”Ÿæˆç»“æ„å·²ä¿å­˜è‡³ shuyuan.json")
    else:
        print(f"ğŸ“š å‘½ä¸­ä¹¦æºï¼š{book_source.get('bookSourceName')}")

    # âœ… è·å–å°è¯´æ ‡é¢˜ä½œä¸ºä¿å­˜å
    novel_title = domain
    try:
        soup = BeautifulSoup(fetch_html(start_url), 'html.parser')
        raw_title = soup.title.string or domain
        novel_title = raw_title.split('_')[0].strip()
    except:
        pass

    writer = ChapterWriter(domain, novel_title)
    current_url = writer.load_progress() or start_url
    visited = set()
    executor = ThreadPoolExecutor(max_workers=2)
    next_future = None

    while current_url and current_url not in visited:
        visited.add(current_url)

        if next_future:
            result = next_future.result()
        else:
            result = scrape_chapter(current_url, book_source)

        if not result:
            print("âŒ æŠ“å–å¤±è´¥ï¼Œç»ˆæ­¢ä»»åŠ¡ã€‚")
            break

        cleaned = clean_content(result['content'])
        writer.save_chapter(result['title'], cleaned)
        writer.save_progress(current_url)
        print(f"âœ… æŠ“å–ï¼š{result['title']}")

        current_url = result['next_url']
        if current_url and current_url not in visited:
            next_future = executor.submit(scrape_chapter, current_url, book_source)
        else:
            break

        time.sleep(1)

    print("ğŸ“˜ æŠ“å–ç»“æŸ")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('url', help='å°è¯´èµ·å§‹ç« èŠ‚çš„URL')
    args = parser.parse_args()
    main(args.url)
