# novel_crawler/main.py
import os
import argparse
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils import (
    get_domain, parse_toc, scrape_chapter,
    fetch_html, verify_content_rule, sanitize_filename
)
from booksource_loader import find_book_source, append_book_source
from ai_analyzer import AIAnalyzer
from chapter_writer import ChapterWriter
from logger import logger


def main(toc_url):
    domain = get_domain(toc_url)
    logger.info(f"[ğŸŒ] ç›®æ ‡ç«™ç‚¹ï¼š{domain}")

    book_source = find_book_source(toc_url)
    chapters = []  # åˆå§‹åŒ–

    if not book_source:
        logger.info("ğŸ“¡ æœªæ‰¾åˆ°ä¹¦æºï¼Œå¯åŠ¨ AI åˆ†æç»“æ„...")
        toc_html = fetch_html(toc_url)
        if not toc_html:
            logger.error(f"âŒ æ— æ³•è·å–ç›®å½•é¡µ HTML: {toc_url}ï¼Œç»ˆæ­¢")
            return

        logger.info("ğŸ•µï¸ æ­£åœ¨çŒœæµ‹ç¬¬ä¸€ç«  URL ä»¥ä¾¿ AI åˆ†æ...")
        toc_soup = BeautifulSoup(toc_html, "html.parser")
        all_links = toc_soup.find_all("a", href=True)
        first_chapter_url = None

        toc_path = urlparse(toc_url).path
        if not toc_path.endswith("/"):
            toc_path += "/"

        blacklist_keywords = [
            "login",
            "register",
            "home",
            "index",
            "top",
            "paihang",
            "rank",
            "user",
            "profile",
            "javascript:",
            "mailto:",
            "about",
            "contact",
            "faq",
        ]

        # ç­–ç•¥1
        for link in all_links:
            href = link.get("href")
            if not href or not href.strip() or href.strip() in ["#", "/"]:
                continue
            if any(kw in href.lower() for kw in blacklist_keywords):
                continue
            abs_url = urljoin(toc_url, href)
            link_path = urlparse(abs_url).path
            if (
                get_domain(abs_url) == domain
                and link_path.startswith(toc_path)
                and link_path != toc_path
            ):
                first_chapter_url = abs_url
                logger.info(
                    f"ğŸ‘ AIåˆ†æï¼š(ç­–ç•¥1) çŒœæµ‹ç¬¬ä¸€ç«  URL ä¸º: {first_chapter_url}"
                )
                break

        # ç­–ç•¥2
        if not first_chapter_url:
            for link in all_links:
                href = link.get("href")
                if not href or not href.strip() or href.strip() in ["#", "/"]:
                    continue
                if any(kw in href.lower() for kw in blacklist_keywords):
                    continue
                abs_url = urljoin(toc_url, href)
                if (
                    get_domain(abs_url) == domain
                    and abs_url != toc_url
                    and get_domain(abs_url) == domain
                    and urlparse(abs_url).path not in ["/", ""]
                ):
                    link_text = link.get_text(strip=True)
                    if (
                        re.search(r"ç¬¬.*[ç« ç« èŠ‚]", link_text)
                        or re.search(r"chapter", link_text, re.I)
                        or re.search(r"^\d+$", link_text)
                    ):
                        first_chapter_url = abs_url
                        logger.info(
                            f"ğŸ‘ AIåˆ†æï¼š(ç­–ç•¥2-å›é€€) çŒœæµ‹ç¬¬ä¸€ç«  URL ä¸º: {first_chapter_url}"
                        )
                        break

        if not first_chapter_url:
            logger.error("âŒ AIåˆ†æï¼šæœªèƒ½åœ¨ç›®å½•é¡µçŒœåˆ°ä»»ä½•æœ‰æ•ˆç« èŠ‚é“¾æ¥ï¼Œç»ˆæ­¢")
            return

        chapter_html = fetch_html(first_chapter_url)
        if not chapter_html:
            logger.error(
                f"âŒ AIåˆ†æï¼šæ— æ³•è·å–çŒœæµ‹çš„ç« èŠ‚é¡µ HTML: {first_chapter_url}ï¼Œç»ˆæ­¢"
            )
            return

        analyzer = AIAnalyzer()
        MAX_RETRIES = 3
        last_error = None
        last_failed_rules = None
        chapters_test = []

        for attempt in range(MAX_RETRIES):
            logger.info(f"ğŸš€ AI åˆ†æå¯åŠ¨... (å°è¯• {attempt + 1}/{MAX_RETRIES})")

            book_source = analyzer.analyze_selectors(
                toc_html,
                chapter_html,
                domain,
                last_failed_rules, # ä¼ å…¥å¤±è´¥çš„è§„åˆ™
                last_error,      # ä¼ å…¥å¤±è´¥çš„åŸå› 
            )

            if not book_source:
                last_error = "AI æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„ JSON"
                last_failed_rules = None
                logger.warning(f"ğŸ§ª AI å°è¯• {attempt + 1} å¤±è´¥: {last_error}")
                continue

            logger.info(f"ğŸ•µï¸ æ­£åœ¨éªŒè¯ AI å°è¯• {attempt + 1} çš„è§„åˆ™...")

            chapters_test = parse_toc(toc_url, book_source)
            if not chapters_test:
                last_error = f"éªŒè¯å¤±è´¥: 'ruleToc' ({book_source.get('ruleToc', {}).get('chapterList')}) æ— æ³•æå–ä»»ä½•ç« èŠ‚ã€‚"
                last_failed_rules = book_source
                logger.warning(f"ğŸ§ª AI å°è¯• {attempt + 1} {last_error}")
                continue

            if not verify_content_rule(chapter_html, book_source):
                last_error = f"éªŒè¯å¤±è´¥: 'ruleContent' ({book_source.get('ruleContent', {}).get('content')}) æ— æ³•åœ¨æ ·æœ¬é¡µé¢æå–åˆ°æ­£æ–‡ã€‚"
                last_failed_rules = book_source
                logger.warning(f"ğŸ§ª AI å°è¯• {attempt + 1} {last_error}")
                continue

            logger.info(f"ğŸ‘ AI è§„åˆ™åœ¨ç¬¬ {attempt + 1} æ¬¡å°è¯•éªŒè¯é€šè¿‡ï¼ (ç›®å½•: {len(chapters_test)} ç« , æ­£æ–‡: OK)")
            last_error = None
            break

        if last_error:
            logger.error(f"âŒ AI åœ¨ {MAX_RETRIES} æ¬¡å°è¯•åä»å¤±è´¥ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
            logger.error(f"âŒ æœ€ç»ˆå¤±è´¥åŸå› : {last_error}")
            return

        append_book_source(book_source)
        logger.info("ğŸ“¥ AIç”Ÿæˆç»“æ„å·²ä¿å­˜è‡³ shuyuan.json")
        chapters = chapters_test

    else:
        logger.info(f"ğŸ“š å‘½ä¸­ä¹¦æºï¼š{book_source.get('bookSourceName')}")
        chapters = parse_toc(toc_url, book_source)

    if not chapters:
        logger.error("âŒ æœªèƒ½æå–åˆ°ç« èŠ‚åˆ—è¡¨")
        return

    try:
        if "toc_html" not in locals():
            toc_html = fetch_html(toc_url)

        if toc_html:
            soup = BeautifulSoup(toc_html, "html.parser")
            title_text = soup.title.string or ""  # è·å– titleï¼Œæˆ–ç©ºå­—ç¬¦ä¸²

            title_parts = re.split(r'[_,|\-ï¼Œ]', title_text)
            novel_title = title_parts[0].strip()

            novel_title = novel_title.replace("ç›®å½•", "").replace("æœ€æ–°ç« èŠ‚åˆ—è¡¨", "").strip()

            if not novel_title:
                novel_title = domain
        else:
            novel_title = domain
    except Exception as e:
        logger.warning(f"âš ï¸ æå–å°è¯´æ ‡é¢˜å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸŸåä½œä¸ºæ ‡é¢˜ã€‚")
        novel_title = domain

    novel_title = sanitize_filename(novel_title)
    if not novel_title:
        novel_title = sanitize_filename(domain)

    writer = ChapterWriter(domain, novel_title)
    last_url = writer.load_checkpoint()
    start_index = 0

    if last_url:
        for idx, ch in enumerate(chapters):
            if ch["url"] == last_url:
                start_index = idx + 1
                break

    logger.info(
        f"ğŸ“– å‡†å¤‡çˆ¬å– {len(chapters) - start_index} ç« ï¼ˆä»ç¬¬ {start_index + 1} ç« å¼€å§‹ï¼‰"
    )

    results_buffer = {}
    current_write_index = start_index

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {
            executor.submit(scrape_chapter, ch["url"], book_source, ch["title"]): i
            for i, ch in enumerate(chapters[start_index:], start=start_index)
        }

        for future in as_completed(futures):
            idx = futures[future]
            ch = chapters[idx]
            try:
                result = future.result()
                if not result:
                    logger.error(f"âŒ æŠ“å–å¤±è´¥ï¼š{ch['title']}ï¼Œç»ˆæ­¢ä»»åŠ¡ã€‚")
                    break

                logger.info(f"âœ… (å·²æŠ“å–) {ch['title']} (Index: {idx})")

                results_buffer[idx] = result

                while current_write_index in results_buffer:
                    chapter_to_write = results_buffer.pop(current_write_index)

                    writer.write_chapters([chapter_to_write])
                    writer.save_checkpoint(chapter_to_write["url"])

                    logger.info(
                        f"ğŸ’¾ (å·²å†™å…¥) {chapter_to_write['title']} (Index: {current_write_index})"
                    )

                    current_write_index += 1

            except Exception as e:
                logger.error(f"âŒ æŠ“å–å¼‚å¸¸ï¼š{ch['title']} - {e}")
                logger.exception(f"è¯¦ç»†é”™è¯¯ (Index: {idx}):")
                break

    logger.info("ğŸ“˜ æŠ“å–æµç¨‹å®Œæˆ")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("url", help="å°è¯´ç›®å½•é¡µ URL")
    args = parser.parse_args()
    main(args.url)